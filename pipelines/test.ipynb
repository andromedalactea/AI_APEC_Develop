{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install  -r ../requirements_pipelines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the base path to process the data\n",
    "# base_path = '/mnt/i'\n",
    "\n",
    "# # Create an Object to process the data\n",
    "# process_data = ProcessData()\n",
    "\n",
    "# # Iterate over all files in a directory\n",
    "# for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "\n",
    "#     # Explore the files\n",
    "#     for filename in filenames:\n",
    "        \n",
    "#         # Extract the extension\n",
    "#         file_extension = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "#         if file_extension == '.pdf':\n",
    "            \n",
    "#             # Process the PDF file\n",
    "#             result = process_data.process_pdf(os.path.join(dirpath, filename))\n",
    "            \n",
    "#             # If the file was processed successfully\n",
    "#             if result == 'Success':\n",
    "#                 # How many files were processed\n",
    "#                 with open(process_data.history_file, 'r') as file:\n",
    "#                     number_files = len(file.read().splitlines())\n",
    "#                     print(\"Number of files processed: \", number_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python Imports\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Third party imports\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData:\n",
    "    def __init__(self,\n",
    "                 data_directory: str =  \"/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001\",\n",
    "                 history_file: str = \"/home/andromedalactea/freelance/AI_APEC_Develop/data/processed_files.txt\",\n",
    "                 error_file: str = \"/home/andromedalactea/freelance/AI_APEC_Develop/data/error_files.txt\",\n",
    "                 embedding_model: str = \"text-embedding-3-large\"):\n",
    "        \n",
    "        # List to store the documents temporarily\n",
    "        self.documents = []\n",
    "\n",
    "        # History file\n",
    "        os.makedirs(os.path.dirname(history_file), exist_ok=True)\n",
    "        self.history_file = history_file\n",
    "\n",
    "        # Error file\n",
    "        os.makedirs(os.path.dirname(error_file), exist_ok=True)\n",
    "        self.error_file = error_file\n",
    "\n",
    "        # Define the Database to store the embeddings\n",
    "        self.data_directory = data_directory\n",
    "\n",
    "        # Load already processed files into memory\n",
    "        self.processed_files = self.load_processed_files()\n",
    "\n",
    "        # Define the initial text splitter\n",
    "        self.text_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\",  # Use line breaks as separator\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "\n",
    "        # Embedding Model\n",
    "        self.embedding_openai = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Verify the data directory\n",
    "        os.makedirs(self.data_directory, exist_ok=True)  # Create the directory and any necessary parent directories\n",
    "        os.chmod(self.data_directory, 0o777) \n",
    "        ## Vector Store Chroma\n",
    "        # Initialize MongoDB python client\n",
    "        client = MongoClient(os.getenv(\"URI_MONGODB_VECTOR_SEARCH\"))\n",
    "\n",
    "        DB_NAME = \"apec_db\"\n",
    "        COLLECTION_NAME = \"apec_vectorstores\"\n",
    "        ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "\n",
    "        MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "        self.vector_store = MongoDBAtlasVectorSearch(\n",
    "            collection=MONGODB_COLLECTION,\n",
    "            embedding=self.embedding_openai,\n",
    "            index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            relevance_score_fn=\"cosine\",\n",
    "        )\n",
    "\n",
    "    def load_processed_files(self):\n",
    "        \"\"\"\n",
    "        Loads the processed files from the history file into a list\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.history_file, 'r') as file:\n",
    "                return set(file.read().splitlines())\n",
    "        except FileNotFoundError:\n",
    "            return set()\n",
    "\n",
    "    def process_pdf(self, filepath: str):\n",
    "        \"\"\"\n",
    "        This Method processes a PDF file and puts the documents into the self.documents list\n",
    "        :param filepath: The path to the PDF file\n",
    "        :return: 'Success' if the file was processed, 'Skipped' if it was already processed\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Verify if the file was already processed\n",
    "            if filepath in self.processed_files:\n",
    "                print(f\"{filepath} already processed. Skipping.\")\n",
    "                return 'Skipped'\n",
    "\n",
    "            # Load the file\n",
    "            loader = UnstructuredPDFLoader(filepath)\n",
    "            data = loader.load()\n",
    "\n",
    "            # Split the text into chunks\n",
    "            documents_pdf = self.text_splitter.split_documents(data)\n",
    "            print(\"Number of documents:\", len(documents_pdf))\n",
    "\n",
    "            # Save the processed data\n",
    "            self.documents.extend(documents_pdf)\n",
    "\n",
    "            # If there are many documents, save and free the memory\n",
    "            if len(self.documents) > 50:\n",
    "                status_save = self.save_procceced_data_into_vector_store()\n",
    "                if status_save == 'Success':\n",
    "                    self.documents = []  # Clear documents to free memory\n",
    "                    print(\"Data was processed and saved\")\n",
    "\n",
    "            # Save the file in the history after processing\n",
    "            self.processed_files.add(filepath)  # Add to in-memory history\n",
    "            with open(self.history_file, 'a') as file:\n",
    "                file.write(filepath + \"\\n\")\n",
    "\n",
    "            return 'Success'\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filepath}: {e}\")\n",
    "            with open(self.error_file, 'a') as file:\n",
    "                file.write(filepath + str(e) + \"\\n\")\n",
    "            return 'Error'\n",
    "\n",
    "    def save_procceced_data_into_vector_store(self):\n",
    "        \"\"\"\n",
    "        Saves the processed data into a vector store\n",
    "        :param documents_to_save: List of processed documents\n",
    "        :return: 'Success' if saved successfully\n",
    "        \"\"\"\n",
    "        # Save the data in the vector store\n",
    "        self.vector_store.add_documents(documents=self.documents)\n",
    "\n",
    "        return 'Success'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the data to a Vector Store Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21018/708988436.py:35: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  self.embedding_openai = OpenAIEmbeddings(model=embedding_model)\n",
      "/home/andromedalactea/freelance/AI_APEC_Develop/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 148\n",
      "Data was processed and saved\n",
      "Number of files processed:  1\n",
      "Number of documents: 36\n",
      "Number of files processed:  2\n",
      "Number of documents: 6\n",
      "Number of files processed:  3\n",
      "Number of documents: 558\n",
      "Data was processed and saved\n",
      "Number of files processed:  4\n",
      "Number of documents: 889\n",
      "Data was processed and saved\n",
      "Number of files processed:  5\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/Imke De Pater, Jack Jonathan Lissauer - Planetary sciences-Cambridge University Press (2001).pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Number of documents: 43\n",
      "Number of files processed:  6\n",
      "Number of documents: 6\n",
      "Number of files processed:  7\n",
      "Number of documents: 149\n",
      "Data was processed and saved\n",
      "Number of files processed:  8\n",
      "Number of documents: 45\n",
      "Number of files processed:  9\n",
      "Number of documents: 889\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/Lucy-Ann McFadden Editor, Paul Weissman Editor,.pdf: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-large in organization org-uuU8cOaIWD5esRRpAuDdxYtA on tokens per min (TPM): Limit 1000000, Requested 1056216. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Number of documents: 412\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/Physics-of-the-Earth-Frank-D.-Stacey-Paul-M.-Davis.pdf: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-large in organization org-uuU8cOaIWD5esRRpAuDdxYtA on tokens per min (TPM): Limit 1000000, Requested 1138773. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Number of documents: 136\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/Destiny or Chance Revisited_ Planets and their .pdf: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-large in organization org-uuU8cOaIWD5esRRpAuDdxYtA on tokens per min (TPM): Limit 1000000, Requested 1138773. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 8803, which is longer than the specified 5000\n",
      "Created a chunk of size 5239, which is longer than the specified 5000\n",
      "Created a chunk of size 7272, which is longer than the specified 5000\n",
      "Created a chunk of size 5179, which is longer than the specified 5000\n",
      "Created a chunk of size 5065, which is longer than the specified 5000\n",
      "Created a chunk of size 5208, which is longer than the specified 5000\n",
      "Created a chunk of size 5677, which is longer than the specified 5000\n",
      "Created a chunk of size 5048, which is longer than the specified 5000\n",
      "Created a chunk of size 6181, which is longer than the specified 5000\n",
      "Created a chunk of size 9376, which is longer than the specified 5000\n",
      "Created a chunk of size 5919, which is longer than the specified 5000\n",
      "Created a chunk of size 5699, which is longer than the specified 5000\n",
      "Created a chunk of size 5320, which is longer than the specified 5000\n",
      "Created a chunk of size 5171, which is longer than the specified 5000\n",
      "Created a chunk of size 9063, which is longer than the specified 5000\n",
      "Created a chunk of size 5680, which is longer than the specified 5000\n",
      "Created a chunk of size 6013, which is longer than the specified 5000\n",
      "Created a chunk of size 6226, which is longer than the specified 5000\n",
      "Created a chunk of size 11519, which is longer than the specified 5000\n",
      "Created a chunk of size 6277, which is longer than the specified 5000\n",
      "Created a chunk of size 7918, which is longer than the specified 5000\n",
      "Created a chunk of size 6484, which is longer than the specified 5000\n",
      "Created a chunk of size 5727, which is longer than the specified 5000\n",
      "Created a chunk of size 6382, which is longer than the specified 5000\n",
      "Created a chunk of size 9114, which is longer than the specified 5000\n",
      "Created a chunk of size 5322, which is longer than the specified 5000\n",
      "Created a chunk of size 6443, which is longer than the specified 5000\n",
      "Created a chunk of size 6284, which is longer than the specified 5000\n",
      "Created a chunk of size 5822, which is longer than the specified 5000\n",
      "Created a chunk of size 11409, which is longer than the specified 5000\n",
      "Created a chunk of size 6214, which is longer than the specified 5000\n",
      "Created a chunk of size 5830, which is longer than the specified 5000\n",
      "Created a chunk of size 6528, which is longer than the specified 5000\n",
      "Created a chunk of size 5122, which is longer than the specified 5000\n",
      "Created a chunk of size 6748, which is longer than the specified 5000\n",
      "Created a chunk of size 6711, which is longer than the specified 5000\n",
      "Created a chunk of size 6177, which is longer than the specified 5000\n",
      "Created a chunk of size 5024, which is longer than the specified 5000\n",
      "Created a chunk of size 11110, which is longer than the specified 5000\n",
      "Created a chunk of size 11416, which is longer than the specified 5000\n",
      "Created a chunk of size 5638, which is longer than the specified 5000\n",
      "Created a chunk of size 7787, which is longer than the specified 5000\n",
      "Created a chunk of size 6339, which is longer than the specified 5000\n",
      "Created a chunk of size 6098, which is longer than the specified 5000\n",
      "Created a chunk of size 5491, which is longer than the specified 5000\n",
      "Created a chunk of size 7321, which is longer than the specified 5000\n",
      "Created a chunk of size 8326, which is longer than the specified 5000\n",
      "Created a chunk of size 6698, which is longer than the specified 5000\n",
      "Created a chunk of size 6921, which is longer than the specified 5000\n",
      "Created a chunk of size 5990, which is longer than the specified 5000\n",
      "Created a chunk of size 7646, which is longer than the specified 5000\n",
      "Created a chunk of size 5515, which is longer than the specified 5000\n",
      "Created a chunk of size 5825, which is longer than the specified 5000\n",
      "Created a chunk of size 7682, which is longer than the specified 5000\n",
      "Created a chunk of size 7205, which is longer than the specified 5000\n",
      "Created a chunk of size 6196, which is longer than the specified 5000\n",
      "Created a chunk of size 5721, which is longer than the specified 5000\n",
      "Created a chunk of size 8468, which is longer than the specified 5000\n",
      "Created a chunk of size 8070, which is longer than the specified 5000\n",
      "Created a chunk of size 7611, which is longer than the specified 5000\n",
      "Created a chunk of size 5163, which is longer than the specified 5000\n",
      "Created a chunk of size 7487, which is longer than the specified 5000\n",
      "Created a chunk of size 5506, which is longer than the specified 5000\n",
      "Created a chunk of size 6982, which is longer than the specified 5000\n",
      "Created a chunk of size 5041, which is longer than the specified 5000\n",
      "Created a chunk of size 5717, which is longer than the specified 5000\n",
      "Created a chunk of size 5447, which is longer than the specified 5000\n",
      "Created a chunk of size 5508, which is longer than the specified 5000\n",
      "Created a chunk of size 5651, which is longer than the specified 5000\n",
      "Created a chunk of size 6336, which is longer than the specified 5000\n",
      "Created a chunk of size 5504, which is longer than the specified 5000\n",
      "Created a chunk of size 7284, which is longer than the specified 5000\n",
      "Created a chunk of size 5004, which is longer than the specified 5000\n",
      "Created a chunk of size 5353, which is longer than the specified 5000\n",
      "Created a chunk of size 5308, which is longer than the specified 5000\n",
      "Created a chunk of size 7150, which is longer than the specified 5000\n",
      "Created a chunk of size 6288, which is longer than the specified 5000\n",
      "Created a chunk of size 5189, which is longer than the specified 5000\n",
      "Created a chunk of size 6232, which is longer than the specified 5000\n",
      "Created a chunk of size 5434, which is longer than the specified 5000\n",
      "Created a chunk of size 8262, which is longer than the specified 5000\n",
      "Created a chunk of size 7130, which is longer than the specified 5000\n",
      "Created a chunk of size 9304, which is longer than the specified 5000\n",
      "Created a chunk of size 7360, which is longer than the specified 5000\n",
      "Created a chunk of size 5443, which is longer than the specified 5000\n",
      "Created a chunk of size 6839, which is longer than the specified 5000\n",
      "Created a chunk of size 5018, which is longer than the specified 5000\n",
      "Created a chunk of size 5102, which is longer than the specified 5000\n",
      "Created a chunk of size 6159, which is longer than the specified 5000\n",
      "Created a chunk of size 7808, which is longer than the specified 5000\n",
      "Created a chunk of size 8144, which is longer than the specified 5000\n",
      "Created a chunk of size 5987, which is longer than the specified 5000\n",
      "Created a chunk of size 6165, which is longer than the specified 5000\n",
      "Created a chunk of size 5769, which is longer than the specified 5000\n",
      "Created a chunk of size 5762, which is longer than the specified 5000\n",
      "Created a chunk of size 6455, which is longer than the specified 5000\n",
      "Created a chunk of size 9754, which is longer than the specified 5000\n",
      "Created a chunk of size 5086, which is longer than the specified 5000\n",
      "Created a chunk of size 6043, which is longer than the specified 5000\n",
      "Created a chunk of size 6255, which is longer than the specified 5000\n",
      "Created a chunk of size 6447, which is longer than the specified 5000\n",
      "Created a chunk of size 5334, which is longer than the specified 5000\n",
      "Created a chunk of size 5240, which is longer than the specified 5000\n",
      "Created a chunk of size 5978, which is longer than the specified 5000\n",
      "Created a chunk of size 5609, which is longer than the specified 5000\n",
      "Created a chunk of size 5717, which is longer than the specified 5000\n",
      "Created a chunk of size 5587, which is longer than the specified 5000\n",
      "Created a chunk of size 9413, which is longer than the specified 5000\n",
      "Created a chunk of size 7147, which is longer than the specified 5000\n",
      "Created a chunk of size 5785, which is longer than the specified 5000\n",
      "Created a chunk of size 9089, which is longer than the specified 5000\n",
      "Created a chunk of size 5084, which is longer than the specified 5000\n",
      "Created a chunk of size 7399, which is longer than the specified 5000\n",
      "Created a chunk of size 7420, which is longer than the specified 5000\n",
      "Created a chunk of size 8283, which is longer than the specified 5000\n",
      "Created a chunk of size 5311, which is longer than the specified 5000\n",
      "Created a chunk of size 7198, which is longer than the specified 5000\n",
      "Created a chunk of size 8721, which is longer than the specified 5000\n",
      "Created a chunk of size 5274, which is longer than the specified 5000\n",
      "Created a chunk of size 5419, which is longer than the specified 5000\n",
      "Created a chunk of size 5887, which is longer than the specified 5000\n",
      "Created a chunk of size 9716, which is longer than the specified 5000\n",
      "Created a chunk of size 5251, which is longer than the specified 5000\n",
      "Created a chunk of size 10825, which is longer than the specified 5000\n",
      "Created a chunk of size 7394, which is longer than the specified 5000\n",
      "Created a chunk of size 7104, which is longer than the specified 5000\n",
      "Created a chunk of size 6204, which is longer than the specified 5000\n",
      "Created a chunk of size 6829, which is longer than the specified 5000\n",
      "Created a chunk of size 5608, which is longer than the specified 5000\n",
      "Created a chunk of size 5285, which is longer than the specified 5000\n",
      "Created a chunk of size 7361, which is longer than the specified 5000\n",
      "Created a chunk of size 7658, which is longer than the specified 5000\n",
      "Created a chunk of size 5132, which is longer than the specified 5000\n",
      "Created a chunk of size 5325, which is longer than the specified 5000\n",
      "Created a chunk of size 8112, which is longer than the specified 5000\n",
      "Created a chunk of size 5251, which is longer than the specified 5000\n",
      "Created a chunk of size 6426, which is longer than the specified 5000\n",
      "Created a chunk of size 5008, which is longer than the specified 5000\n",
      "Created a chunk of size 5029, which is longer than the specified 5000\n",
      "Created a chunk of size 6638, which is longer than the specified 5000\n",
      "Created a chunk of size 5495, which is longer than the specified 5000\n",
      "Created a chunk of size 5620, which is longer than the specified 5000\n",
      "Created a chunk of size 5061, which is longer than the specified 5000\n",
      "Created a chunk of size 5353, which is longer than the specified 5000\n",
      "Created a chunk of size 6660, which is longer than the specified 5000\n",
      "Created a chunk of size 5963, which is longer than the specified 5000\n",
      "Created a chunk of size 6728, which is longer than the specified 5000\n",
      "Created a chunk of size 5369, which is longer than the specified 5000\n",
      "Created a chunk of size 5087, which is longer than the specified 5000\n",
      "Created a chunk of size 8505, which is longer than the specified 5000\n",
      "Created a chunk of size 5891, which is longer than the specified 5000\n",
      "Created a chunk of size 10296, which is longer than the specified 5000\n",
      "Created a chunk of size 6817, which is longer than the specified 5000\n",
      "Created a chunk of size 9598, which is longer than the specified 5000\n",
      "Created a chunk of size 5910, which is longer than the specified 5000\n",
      "Created a chunk of size 6339, which is longer than the specified 5000\n",
      "Created a chunk of size 5742, which is longer than the specified 5000\n",
      "Created a chunk of size 12139, which is longer than the specified 5000\n",
      "Created a chunk of size 8305, which is longer than the specified 5000\n",
      "Created a chunk of size 6580, which is longer than the specified 5000\n",
      "Created a chunk of size 5955, which is longer than the specified 5000\n",
      "Created a chunk of size 7166, which is longer than the specified 5000\n",
      "Created a chunk of size 5513, which is longer than the specified 5000\n",
      "Created a chunk of size 5488, which is longer than the specified 5000\n",
      "Created a chunk of size 9988, which is longer than the specified 5000\n",
      "Created a chunk of size 5537, which is longer than the specified 5000\n",
      "Created a chunk of size 5281, which is longer than the specified 5000\n",
      "Created a chunk of size 7170, which is longer than the specified 5000\n",
      "Created a chunk of size 5623, which is longer than the specified 5000\n",
      "Created a chunk of size 9337, which is longer than the specified 5000\n",
      "Created a chunk of size 6812, which is longer than the specified 5000\n",
      "Created a chunk of size 5364, which is longer than the specified 5000\n",
      "Created a chunk of size 5262, which is longer than the specified 5000\n",
      "Created a chunk of size 5751, which is longer than the specified 5000\n",
      "Created a chunk of size 7321, which is longer than the specified 5000\n",
      "Created a chunk of size 5142, which is longer than the specified 5000\n",
      "Created a chunk of size 6893, which is longer than the specified 5000\n",
      "Created a chunk of size 6997, which is longer than the specified 5000\n",
      "Created a chunk of size 5009, which is longer than the specified 5000\n",
      "Created a chunk of size 8236, which is longer than the specified 5000\n",
      "Created a chunk of size 5323, which is longer than the specified 5000\n",
      "Created a chunk of size 5566, which is longer than the specified 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1928\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/An Introduction to Modern Astrophysics by Bradley W. Carroll, Dale A. Ostlie (z-lib.org).pdf: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-large in organization org-uuU8cOaIWD5esRRpAuDdxYtA on tokens per min (TPM): Limit 1000000, Requested 1138773. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "# Define the base path to process the data\n",
    "base_path = '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001'\n",
    "\n",
    "# Create an Object to process the data\n",
    "process_data = ProcessData()\n",
    "\n",
    "# Iterate over all files in a directory\n",
    "for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "\n",
    "    # Explore the files\n",
    "    for filename in filenames:\n",
    "        \n",
    "        # Extract the extension\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "        if file_extension == '.pdf':\n",
    "            \n",
    "            # Process the PDF file\n",
    "            result = process_data.process_pdf(os.path.join(dirpath, filename))\n",
    "            \n",
    "            # If the file was processed successfully\n",
    "            if result == 'Success':\n",
    "                # How many files were processed\n",
    "                with open(process_data.history_file, 'r') as file:\n",
    "                    number_files = len(file.read().splitlines())\n",
    "                    print(\"Number of files processed: \", number_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=1.502466] \n",
      "Additional Slides \n",
      "[{'page': 45, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}]\n",
      "* [SIM=1.597223] \n",
      "QuickTime™ and a\n",
      "Cinepak decompressor\n",
      "are needed to see this picture. \n",
      "[{'page': 19, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}]\n",
      "* [SIM=1.637236] \n",
      "Composition of condensable \n",
      "material\n",
      "Feeding zone: origin of planetary material \n",
      "[{'page': 6, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}]\n",
      "* [SIM=1.637690] \n",
      "Additional InformationAdditional Information\n",
      "•sean.raymond@colorado.edu \n",
      "[{'page': 44, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}]\n",
      "[(Document(metadata={'page': 45, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}, page_content='Additional Slides'), 1.5024656074974443), (Document(metadata={'page': 19, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}, page_content='QuickTime™ and a\\nCinepak decompressor\\nare needed to see this picture.'), 1.597223034948216), (Document(metadata={'page': 6, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}, page_content='Composition of condensable \\nmaterial\\nFeeding zone: origin of planetary material'), 1.6372364034541949), (Document(metadata={'page': 44, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}, page_content='Additional InformationAdditional Information\\n•sean.raymond@colorado.edu'), 1.637690332812858)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_store = Chroma(\n",
    "                    collection_name=\"apec_vectorstores\",\n",
    "                    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "                    persist_directory=\"/home/andromedalactea/freelance/AI_APEC_Develop/data/APEC_ChromaDB\",  \n",
    "                )\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"How to create a car wash media set\", k=4\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] \\n{res.page_content} \\n[{res.metadata}]\")\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
