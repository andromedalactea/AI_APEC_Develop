{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install  -r ../requirements_pipelines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the base path to process the data\n",
    "# base_path = '/mnt/i'\n",
    "\n",
    "# # Create an Object to process the data\n",
    "# process_data = ProcessData()\n",
    "\n",
    "# # Iterate over all files in a directory\n",
    "# for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "\n",
    "#     # Explore the files\n",
    "#     for filename in filenames:\n",
    "        \n",
    "#         # Extract the extension\n",
    "#         file_extension = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "#         if file_extension == '.pdf':\n",
    "            \n",
    "#             # Process the PDF file\n",
    "#             result = process_data.process_pdf(os.path.join(dirpath, filename))\n",
    "            \n",
    "#             # If the file was processed successfully\n",
    "#             if result == 'Success':\n",
    "#                 # How many files were processed\n",
    "#                 with open(process_data.history_file, 'r') as file:\n",
    "#                     number_files = len(file.read().splitlines())\n",
    "#                     print(\"Number of files processed: \", number_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python Imports\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Third party imports\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData:\n",
    "    def __init__(self,\n",
    "                 data_directory: str =  \"/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001\",\n",
    "                 history_file: str = \"/home/andromedalactea/freelance/AI_APEC_Develop/data/processed_files.txt\",\n",
    "                 error_file: str = \"/home/andromedalactea/freelance/AI_APEC_Develop/data/error_files.txt\",\n",
    "                 embedding_model: str = \"text-embedding-3-large\"):\n",
    "        \n",
    "        # List to store the documents temporarily\n",
    "        self.documents = []\n",
    "\n",
    "        # History file\n",
    "        os.makedirs(os.path.dirname(history_file), exist_ok=True)\n",
    "        self.history_file = history_file\n",
    "\n",
    "        # Error file\n",
    "        os.makedirs(os.path.dirname(error_file), exist_ok=True)\n",
    "        self.error_file = error_file\n",
    "\n",
    "        # Define the Database to store the embeddings\n",
    "        self.data_directory = data_directory\n",
    "\n",
    "        # Load already processed files into memory\n",
    "        self.processed_files = self.load_processed_files()\n",
    "\n",
    "        # Define the initial text splitter\n",
    "        self.text_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\",  # Use line breaks as separator\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "\n",
    "        # Embedding Model\n",
    "        self.embedding_openai = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Verify the data directory\n",
    "        os.makedirs(self.data_directory, exist_ok=True)  # Create the directory and any necessary parent directories\n",
    "        os.chmod(self.data_directory, 0o777) \n",
    "        ## Vector Store Chroma\n",
    "        # Initialize MongoDB python client\n",
    "        client = MongoClient(os.getenv(\"URI_MONGODB_VECTOR_SEARCH\"))\n",
    "\n",
    "        DB_NAME = \"apec_db\"\n",
    "        COLLECTION_NAME = \"apec_vectorstores\"\n",
    "        ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "\n",
    "        MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "        self.vector_store = MongoDBAtlasVectorSearch(\n",
    "            collection=MONGODB_COLLECTION,\n",
    "            embedding=self.embedding_openai,\n",
    "            index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            relevance_score_fn=\"cosine\",\n",
    "        )\n",
    "\n",
    "    def load_processed_files(self):\n",
    "        \"\"\"\n",
    "        Loads the processed files from the history file into a list\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.history_file, 'r') as file:\n",
    "                return set(file.read().splitlines())\n",
    "        except FileNotFoundError:\n",
    "            return set()\n",
    "\n",
    "    def process_pdf(self, filepath: str):\n",
    "        \"\"\"\n",
    "        This Method processes a PDF file and puts the documents into the self.documents list\n",
    "        :param filepath: The path to the PDF file\n",
    "        :return: 'Success' if the file was processed, 'Skipped' if it was already processed\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Verify if the file was already processed\n",
    "            if filepath in self.processed_files:\n",
    "                print(f\"{filepath} already processed. Skipping.\")\n",
    "                return 'Skipped'\n",
    "\n",
    "            # Load the file\n",
    "            loader = UnstructuredPDFLoader(filepath)\n",
    "            data = loader.load()\n",
    "\n",
    "            # Split the text into chunks\n",
    "            documents_pdf = self.text_splitter.split_documents(data)\n",
    "            print(\"Number of documents:\", len(documents_pdf))\n",
    "\n",
    "            # Save the processed data\n",
    "            self.documents.extend(documents_pdf)\n",
    "\n",
    "            # If there are many documents, save and free the memory\n",
    "            if len(self.documents) > 50:\n",
    "                status_save = self.save_procceced_data_into_vector_store()\n",
    "                if status_save == 'Success':\n",
    "                    self.documents = []  # Clear documents to free memory\n",
    "                    print(\"Data was processed and saved\")\n",
    "\n",
    "            # Save the file in the history after processing\n",
    "            self.processed_files.add(filepath)  # Add to in-memory history\n",
    "            with open(self.history_file, 'a') as file:\n",
    "                file.write(filepath + \"\\n\")\n",
    "\n",
    "            return 'Success'\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filepath}: {e}\")\n",
    "            with open(self.error_file, 'a') as file:\n",
    "                file.write(filepath + str(e) + \"\\n\")\n",
    "            return 'Error'\n",
    "\n",
    "    def save_procceced_data_into_vector_store(self):\n",
    "        \"\"\"\n",
    "        Saves the processed data into a vector store\n",
    "        :param documents_to_save: List of processed documents\n",
    "        :return: 'Success' if saved successfully\n",
    "        \"\"\"\n",
    "        # Save the data in the vector store\n",
    "        self.vector_store.add_documents(documents=self.documents)\n",
    "\n",
    "        return 'Success'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the data to a Vector Store Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21018/708988436.py:35: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  self.embedding_openai = OpenAIEmbeddings(model=embedding_model)\n",
      "/home/andromedalactea/freelance/AI_APEC_Develop/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 148\n",
      "Data was processed and saved\n",
      "Number of files processed:  1\n",
      "Number of documents: 36\n",
      "Number of files processed:  2\n",
      "Number of documents: 6\n",
      "Number of files processed:  3\n",
      "Number of documents: 558\n",
      "Data was processed and saved\n",
      "Number of files processed:  4\n",
      "Number of documents: 889\n",
      "Data was processed and saved\n",
      "Number of files processed:  5\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/Imke De Pater, Jack Jonathan Lissauer - Planetary sciences-Cambridge University Press (2001).pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Number of documents: 43\n",
      "Number of files processed:  6\n",
      "Number of documents: 6\n",
      "Number of files processed:  7\n",
      "Number of documents: 149\n",
      "Data was processed and saved\n",
      "Number of files processed:  8\n",
      "Number of documents: 45\n",
      "Number of files processed:  9\n",
      "Number of documents: 889\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/Lucy-Ann McFadden Editor, Paul Weissman Editor,.pdf: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-large in organization org-uuU8cOaIWD5esRRpAuDdxYtA on tokens per min (TPM): Limit 1000000, Requested 1056216. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Number of documents: 412\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/Physics-of-the-Earth-Frank-D.-Stacey-Paul-M.-Davis.pdf: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-large in organization org-uuU8cOaIWD5esRRpAuDdxYtA on tokens per min (TPM): Limit 1000000, Requested 1138773. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Number of documents: 136\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/Destiny or Chance Revisited_ Planets and their .pdf: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-large in organization org-uuU8cOaIWD5esRRpAuDdxYtA on tokens per min (TPM): Limit 1000000, Requested 1138773. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 8803, which is longer than the specified 5000\n",
      "Created a chunk of size 5239, which is longer than the specified 5000\n",
      "Created a chunk of size 7272, which is longer than the specified 5000\n",
      "Created a chunk of size 5179, which is longer than the specified 5000\n",
      "Created a chunk of size 5065, which is longer than the specified 5000\n",
      "Created a chunk of size 5208, which is longer than the specified 5000\n",
      "Created a chunk of size 5677, which is longer than the specified 5000\n",
      "Created a chunk of size 5048, which is longer than the specified 5000\n",
      "Created a chunk of size 6181, which is longer than the specified 5000\n",
      "Created a chunk of size 9376, which is longer than the specified 5000\n",
      "Created a chunk of size 5919, which is longer than the specified 5000\n",
      "Created a chunk of size 5699, which is longer than the specified 5000\n",
      "Created a chunk of size 5320, which is longer than the specified 5000\n",
      "Created a chunk of size 5171, which is longer than the specified 5000\n",
      "Created a chunk of size 9063, which is longer than the specified 5000\n",
      "Created a chunk of size 5680, which is longer than the specified 5000\n",
      "Created a chunk of size 6013, which is longer than the specified 5000\n",
      "Created a chunk of size 6226, which is longer than the specified 5000\n",
      "Created a chunk of size 11519, which is longer than the specified 5000\n",
      "Created a chunk of size 6277, which is longer than the specified 5000\n",
      "Created a chunk of size 7918, which is longer than the specified 5000\n",
      "Created a chunk of size 6484, which is longer than the specified 5000\n",
      "Created a chunk of size 5727, which is longer than the specified 5000\n",
      "Created a chunk of size 6382, which is longer than the specified 5000\n",
      "Created a chunk of size 9114, which is longer than the specified 5000\n",
      "Created a chunk of size 5322, which is longer than the specified 5000\n",
      "Created a chunk of size 6443, which is longer than the specified 5000\n",
      "Created a chunk of size 6284, which is longer than the specified 5000\n",
      "Created a chunk of size 5822, which is longer than the specified 5000\n",
      "Created a chunk of size 11409, which is longer than the specified 5000\n",
      "Created a chunk of size 6214, which is longer than the specified 5000\n",
      "Created a chunk of size 5830, which is longer than the specified 5000\n",
      "Created a chunk of size 6528, which is longer than the specified 5000\n",
      "Created a chunk of size 5122, which is longer than the specified 5000\n",
      "Created a chunk of size 6748, which is longer than the specified 5000\n",
      "Created a chunk of size 6711, which is longer than the specified 5000\n",
      "Created a chunk of size 6177, which is longer than the specified 5000\n",
      "Created a chunk of size 5024, which is longer than the specified 5000\n",
      "Created a chunk of size 11110, which is longer than the specified 5000\n",
      "Created a chunk of size 11416, which is longer than the specified 5000\n",
      "Created a chunk of size 5638, which is longer than the specified 5000\n",
      "Created a chunk of size 7787, which is longer than the specified 5000\n",
      "Created a chunk of size 6339, which is longer than the specified 5000\n",
      "Created a chunk of size 6098, which is longer than the specified 5000\n",
      "Created a chunk of size 5491, which is longer than the specified 5000\n",
      "Created a chunk of size 7321, which is longer than the specified 5000\n",
      "Created a chunk of size 8326, which is longer than the specified 5000\n",
      "Created a chunk of size 6698, which is longer than the specified 5000\n",
      "Created a chunk of size 6921, which is longer than the specified 5000\n",
      "Created a chunk of size 5990, which is longer than the specified 5000\n",
      "Created a chunk of size 7646, which is longer than the specified 5000\n",
      "Created a chunk of size 5515, which is longer than the specified 5000\n",
      "Created a chunk of size 5825, which is longer than the specified 5000\n",
      "Created a chunk of size 7682, which is longer than the specified 5000\n",
      "Created a chunk of size 7205, which is longer than the specified 5000\n",
      "Created a chunk of size 6196, which is longer than the specified 5000\n",
      "Created a chunk of size 5721, which is longer than the specified 5000\n",
      "Created a chunk of size 8468, which is longer than the specified 5000\n",
      "Created a chunk of size 8070, which is longer than the specified 5000\n",
      "Created a chunk of size 7611, which is longer than the specified 5000\n",
      "Created a chunk of size 5163, which is longer than the specified 5000\n",
      "Created a chunk of size 7487, which is longer than the specified 5000\n",
      "Created a chunk of size 5506, which is longer than the specified 5000\n",
      "Created a chunk of size 6982, which is longer than the specified 5000\n",
      "Created a chunk of size 5041, which is longer than the specified 5000\n",
      "Created a chunk of size 5717, which is longer than the specified 5000\n",
      "Created a chunk of size 5447, which is longer than the specified 5000\n",
      "Created a chunk of size 5508, which is longer than the specified 5000\n",
      "Created a chunk of size 5651, which is longer than the specified 5000\n",
      "Created a chunk of size 6336, which is longer than the specified 5000\n",
      "Created a chunk of size 5504, which is longer than the specified 5000\n",
      "Created a chunk of size 7284, which is longer than the specified 5000\n",
      "Created a chunk of size 5004, which is longer than the specified 5000\n",
      "Created a chunk of size 5353, which is longer than the specified 5000\n",
      "Created a chunk of size 5308, which is longer than the specified 5000\n",
      "Created a chunk of size 7150, which is longer than the specified 5000\n",
      "Created a chunk of size 6288, which is longer than the specified 5000\n",
      "Created a chunk of size 5189, which is longer than the specified 5000\n",
      "Created a chunk of size 6232, which is longer than the specified 5000\n",
      "Created a chunk of size 5434, which is longer than the specified 5000\n",
      "Created a chunk of size 8262, which is longer than the specified 5000\n",
      "Created a chunk of size 7130, which is longer than the specified 5000\n",
      "Created a chunk of size 9304, which is longer than the specified 5000\n",
      "Created a chunk of size 7360, which is longer than the specified 5000\n",
      "Created a chunk of size 5443, which is longer than the specified 5000\n",
      "Created a chunk of size 6839, which is longer than the specified 5000\n",
      "Created a chunk of size 5018, which is longer than the specified 5000\n",
      "Created a chunk of size 5102, which is longer than the specified 5000\n",
      "Created a chunk of size 6159, which is longer than the specified 5000\n",
      "Created a chunk of size 7808, which is longer than the specified 5000\n",
      "Created a chunk of size 8144, which is longer than the specified 5000\n",
      "Created a chunk of size 5987, which is longer than the specified 5000\n",
      "Created a chunk of size 6165, which is longer than the specified 5000\n",
      "Created a chunk of size 5769, which is longer than the specified 5000\n",
      "Created a chunk of size 5762, which is longer than the specified 5000\n",
      "Created a chunk of size 6455, which is longer than the specified 5000\n",
      "Created a chunk of size 9754, which is longer than the specified 5000\n",
      "Created a chunk of size 5086, which is longer than the specified 5000\n",
      "Created a chunk of size 6043, which is longer than the specified 5000\n",
      "Created a chunk of size 6255, which is longer than the specified 5000\n",
      "Created a chunk of size 6447, which is longer than the specified 5000\n",
      "Created a chunk of size 5334, which is longer than the specified 5000\n",
      "Created a chunk of size 5240, which is longer than the specified 5000\n",
      "Created a chunk of size 5978, which is longer than the specified 5000\n",
      "Created a chunk of size 5609, which is longer than the specified 5000\n",
      "Created a chunk of size 5717, which is longer than the specified 5000\n",
      "Created a chunk of size 5587, which is longer than the specified 5000\n",
      "Created a chunk of size 9413, which is longer than the specified 5000\n",
      "Created a chunk of size 7147, which is longer than the specified 5000\n",
      "Created a chunk of size 5785, which is longer than the specified 5000\n",
      "Created a chunk of size 9089, which is longer than the specified 5000\n",
      "Created a chunk of size 5084, which is longer than the specified 5000\n",
      "Created a chunk of size 7399, which is longer than the specified 5000\n",
      "Created a chunk of size 7420, which is longer than the specified 5000\n",
      "Created a chunk of size 8283, which is longer than the specified 5000\n",
      "Created a chunk of size 5311, which is longer than the specified 5000\n",
      "Created a chunk of size 7198, which is longer than the specified 5000\n",
      "Created a chunk of size 8721, which is longer than the specified 5000\n",
      "Created a chunk of size 5274, which is longer than the specified 5000\n",
      "Created a chunk of size 5419, which is longer than the specified 5000\n",
      "Created a chunk of size 5887, which is longer than the specified 5000\n",
      "Created a chunk of size 9716, which is longer than the specified 5000\n",
      "Created a chunk of size 5251, which is longer than the specified 5000\n",
      "Created a chunk of size 10825, which is longer than the specified 5000\n",
      "Created a chunk of size 7394, which is longer than the specified 5000\n",
      "Created a chunk of size 7104, which is longer than the specified 5000\n",
      "Created a chunk of size 6204, which is longer than the specified 5000\n",
      "Created a chunk of size 6829, which is longer than the specified 5000\n",
      "Created a chunk of size 5608, which is longer than the specified 5000\n",
      "Created a chunk of size 5285, which is longer than the specified 5000\n",
      "Created a chunk of size 7361, which is longer than the specified 5000\n",
      "Created a chunk of size 7658, which is longer than the specified 5000\n",
      "Created a chunk of size 5132, which is longer than the specified 5000\n",
      "Created a chunk of size 5325, which is longer than the specified 5000\n",
      "Created a chunk of size 8112, which is longer than the specified 5000\n",
      "Created a chunk of size 5251, which is longer than the specified 5000\n",
      "Created a chunk of size 6426, which is longer than the specified 5000\n",
      "Created a chunk of size 5008, which is longer than the specified 5000\n",
      "Created a chunk of size 5029, which is longer than the specified 5000\n",
      "Created a chunk of size 6638, which is longer than the specified 5000\n",
      "Created a chunk of size 5495, which is longer than the specified 5000\n",
      "Created a chunk of size 5620, which is longer than the specified 5000\n",
      "Created a chunk of size 5061, which is longer than the specified 5000\n",
      "Created a chunk of size 5353, which is longer than the specified 5000\n",
      "Created a chunk of size 6660, which is longer than the specified 5000\n",
      "Created a chunk of size 5963, which is longer than the specified 5000\n",
      "Created a chunk of size 6728, which is longer than the specified 5000\n",
      "Created a chunk of size 5369, which is longer than the specified 5000\n",
      "Created a chunk of size 5087, which is longer than the specified 5000\n",
      "Created a chunk of size 8505, which is longer than the specified 5000\n",
      "Created a chunk of size 5891, which is longer than the specified 5000\n",
      "Created a chunk of size 10296, which is longer than the specified 5000\n",
      "Created a chunk of size 6817, which is longer than the specified 5000\n",
      "Created a chunk of size 9598, which is longer than the specified 5000\n",
      "Created a chunk of size 5910, which is longer than the specified 5000\n",
      "Created a chunk of size 6339, which is longer than the specified 5000\n",
      "Created a chunk of size 5742, which is longer than the specified 5000\n",
      "Created a chunk of size 12139, which is longer than the specified 5000\n",
      "Created a chunk of size 8305, which is longer than the specified 5000\n",
      "Created a chunk of size 6580, which is longer than the specified 5000\n",
      "Created a chunk of size 5955, which is longer than the specified 5000\n",
      "Created a chunk of size 7166, which is longer than the specified 5000\n",
      "Created a chunk of size 5513, which is longer than the specified 5000\n",
      "Created a chunk of size 5488, which is longer than the specified 5000\n",
      "Created a chunk of size 9988, which is longer than the specified 5000\n",
      "Created a chunk of size 5537, which is longer than the specified 5000\n",
      "Created a chunk of size 5281, which is longer than the specified 5000\n",
      "Created a chunk of size 7170, which is longer than the specified 5000\n",
      "Created a chunk of size 5623, which is longer than the specified 5000\n",
      "Created a chunk of size 9337, which is longer than the specified 5000\n",
      "Created a chunk of size 6812, which is longer than the specified 5000\n",
      "Created a chunk of size 5364, which is longer than the specified 5000\n",
      "Created a chunk of size 5262, which is longer than the specified 5000\n",
      "Created a chunk of size 5751, which is longer than the specified 5000\n",
      "Created a chunk of size 7321, which is longer than the specified 5000\n",
      "Created a chunk of size 5142, which is longer than the specified 5000\n",
      "Created a chunk of size 6893, which is longer than the specified 5000\n",
      "Created a chunk of size 6997, which is longer than the specified 5000\n",
      "Created a chunk of size 5009, which is longer than the specified 5000\n",
      "Created a chunk of size 8236, which is longer than the specified 5000\n",
      "Created a chunk of size 5323, which is longer than the specified 5000\n",
      "Created a chunk of size 5566, which is longer than the specified 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1928\n",
      "Error processing file /home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/An Introduction to Modern Astrophysics by Bradley W. Carroll, Dale A. Ostlie (z-lib.org).pdf: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-large in organization org-uuU8cOaIWD5esRRpAuDdxYtA on tokens per min (TPM): Limit 1000000, Requested 1138773. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "# Define the base path to process the data\n",
    "base_path = '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001'\n",
    "\n",
    "# Create an Object to process the data\n",
    "process_data = ProcessData()\n",
    "\n",
    "# Iterate over all files in a directory\n",
    "for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "\n",
    "    # Explore the files\n",
    "    for filename in filenames:\n",
    "        \n",
    "        # Extract the extension\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "        if file_extension == '.pdf':\n",
    "            \n",
    "            # Process the PDF file\n",
    "            result = process_data.process_pdf(os.path.join(dirpath, filename))\n",
    "            \n",
    "            # If the file was processed successfully\n",
    "            if result == 'Success':\n",
    "                # How many files were processed\n",
    "                with open(process_data.history_file, 'r') as file:\n",
    "                    number_files = len(file.read().splitlines())\n",
    "                    print(\"Number of files processed: \", number_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=1.502466] \n",
      "Additional Slides \n",
      "[{'page': 45, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}]\n",
      "* [SIM=1.597223] \n",
      "QuickTime™ and a\n",
      "Cinepak decompressor\n",
      "are needed to see this picture. \n",
      "[{'page': 19, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}]\n",
      "* [SIM=1.637236] \n",
      "Composition of condensable \n",
      "material\n",
      "Feeding zone: origin of planetary material \n",
      "[{'page': 6, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}]\n",
      "* [SIM=1.637690] \n",
      "Additional InformationAdditional Information\n",
      "•sean.raymond@colorado.edu \n",
      "[{'page': 44, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}]\n",
      "[(Document(metadata={'page': 45, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}, page_content='Additional Slides'), 1.5024656074974443), (Document(metadata={'page': 19, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}, page_content='QuickTime™ and a\\nCinepak decompressor\\nare needed to see this picture.'), 1.597223034948216), (Document(metadata={'page': 6, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}, page_content='Composition of condensable \\nmaterial\\nFeeding zone: origin of planetary material'), 1.6372364034541949), (Document(metadata={'page': 44, 'source': '/home/andromedalactea/freelance/AI_APEC_Develop/data/Books-20240918T233426Z-001/Books/RockyPlanetsFormation.pdf', 'type_data': 'text'}, page_content='Additional InformationAdditional Information\\n•sean.raymond@colorado.edu'), 1.637690332812858)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_store = Chroma(\n",
    "                    collection_name=\"apec_vectorstores\",\n",
    "                    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "                    persist_directory=\"/home/andromedalactea/freelance/AI_APEC_Develop/data/APEC_ChromaDB\",  \n",
    "                )\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"How to create a car wash media set\", k=4\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] \\n{res.page_content} \\n[{res.metadata}]\")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andromedalactea/freelance/AI_APEC_Develop/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 263\u001b[0m\n\u001b[1;32m    260\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/andromedalactea/freelance/AI_APEC_Develop/data/pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Create an Object to process the data\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m process_data \u001b[38;5;241m=\u001b[39m \u001b[43mProcessData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m base_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18m__file__\u001b[39m)), base_path))\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# List of file extensions that can be converted to PDF\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m, in \u001b[0;36mProcessData.__init__\u001b[0;34m(self, data_directory, history_file, error_file, embedding_model)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# History file\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_file \u001b[38;5;241m=\u001b[39m \u001b[43mabsolute_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_file), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Error file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mabsolute_path\u001b[0;34m(relative_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabsolute_path\u001b[39m(relative_path):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18;43m__file__\u001b[39;49m)), relative_path))\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Python Imports\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# Third party imports\n",
    "from dotenv import load_dotenv\n",
    "import pypandoc\n",
    "from docx2pdf import convert as docx_to_pdf\n",
    "import subprocess\n",
    "import platform\n",
    "import traceback\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def absolute_path(relative_path):\n",
    "    return os.path.normpath(os.path.join(os.path.dirname(os.path.realpath(__file__)), relative_path))\n",
    "\n",
    "class ProcessData:\n",
    "    def __init__(self,\n",
    "                 data_directory: str =  \"../data/APEC_ChromaDB\",\n",
    "                 history_file: str = \"../data/processed_files.txt\",\n",
    "                 error_file: str = \"../data/error_files.txt\",\n",
    "                 embedding_model: str = \"text-embedding-3-small\"):\n",
    "        \n",
    "        # List to store the documents temporarily\n",
    "        self.documents = []\n",
    "\n",
    "        # History file\n",
    "        self.history_file = absolute_path(history_file)\n",
    "        os.makedirs(os.path.dirname(self.history_file), exist_ok=True)\n",
    "\n",
    "        # Error file\n",
    "        self.error_file = absolute_path(error_file)\n",
    "        os.makedirs(os.path.dirname(self.error_file), exist_ok=True)\n",
    "        \n",
    "\n",
    "        # Define the Database to store the embeddings\n",
    "        self.data_directory = absolute_path(data_directory)\n",
    "\n",
    "        # Load already processed files into memory\n",
    "        self.processed_files = self.load_processed_files()\n",
    "\n",
    "        # Define the initial text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                                                            # Set a really small chunk size, just to show.\n",
    "                                                            chunk_size=100,\n",
    "                                                            chunk_overlap=20,\n",
    "                                                            length_function=len,\n",
    "                                                            is_separator_regex=False,\n",
    "                                                        )\n",
    "\n",
    "        # Embedding Model\n",
    "        self.embedding_openai = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Verify the data directory\n",
    "        os.makedirs(self.data_directory, exist_ok=True)  # Create the directory and any necessary parent directories\n",
    "        \n",
    "        # Define the vector store\n",
    "        self.vector_store = Chroma(\n",
    "                    collection_name=\"apec_vectorstores\",\n",
    "                    embedding_function=self.embedding_openai,\n",
    "                    persist_directory=self.data_directory,  \n",
    "                )\n",
    "\n",
    "    def load_processed_files(self):\n",
    "        \"\"\"\n",
    "        Loads the processed files from the history file into a list\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.history_file, 'r') as file:\n",
    "                return set(file.read().splitlines())\n",
    "        except FileNotFoundError:\n",
    "            return set()\n",
    "        \n",
    "    def filter_documents(self, documents: list):\n",
    "        \"\"\"\n",
    "        Filters the documents based on the length and the number of words\n",
    "        \"\"\"\n",
    "        filtered_documents = []\n",
    "\n",
    "        def contains_only_numbers_and_symbols(text):\n",
    "            \"\"\"Verifica si el texto contiene solo números y símbolos (sin caracteres alfabéticos).\"\"\"\n",
    "            return not any(char.isalpha() for char in text)\n",
    "\n",
    "        for doc in documents:\n",
    "            # Extract the page content\n",
    "            page_content = doc.page_content\n",
    "\n",
    "            if (contains_only_numbers_and_symbols(page_content) or \n",
    "                len(page_content) < 25):\n",
    "                continue\n",
    "            else:\n",
    "                filtered_documents.append(doc)\n",
    "\n",
    "        return filtered_documents\n",
    "\n",
    "    def add_metadata(self, documents: list, type: str) -> list:\n",
    "        \"\"\"\n",
    "        Adds metadata to the documents\n",
    "        \"\"\"\n",
    "        for doc in documents:\n",
    "            doc.metadata['type_data'] = type\n",
    "\n",
    "        return documents\n",
    "    \n",
    "    def process_pdf(self, filepath: str):\n",
    "        \"\"\"\n",
    "        This Method processes a PDF file and puts the documents into the self.documents list\n",
    "        :param filepath: The path to the PDF file\n",
    "        :return: 'Success' if the file was processed, 'Skipped' if it was already processed\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Verify if the file was already processed\n",
    "            if filepath in self.processed_files:\n",
    "                print(f\"{filepath} already processed. Skipping.\")\n",
    "                return 'Skipped'\n",
    "\n",
    "            # Load the file\n",
    "            loader = PyPDFLoader(filepath)\n",
    "            data = loader.load()\n",
    "\n",
    "            # Split the text into chunks\n",
    "            documents_pdf = self.text_splitter.split_documents(data)\n",
    "            print(\"Number of documents:\", len(documents_pdf))\n",
    "\n",
    "            # Filter the documents\n",
    "            documents_pdf = self.filter_documents(documents_pdf)\n",
    "\n",
    "            # Add metadata to the documents\n",
    "            documents_pdf = self.add_metadata(documents_pdf, type='text')\n",
    "            print(documents_pdf)\n",
    "            if not documents_pdf:\n",
    "                return False\n",
    "            \n",
    "            # Save the processed data\n",
    "            self.documents.extend(documents_pdf)\n",
    "\n",
    "            # If there are many documents, save and free the memory\n",
    "            if len(self.documents) > 50:\n",
    "                status_save = self.save_procceced_data_into_vector_store()\n",
    "                if status_save == 'Success':\n",
    "                    self.documents = []  # Clear documents to free memory\n",
    "                    print(\"Data was processed and saved\")\n",
    "\n",
    "            # Save the file in the history after processing\n",
    "            self.processed_files.add(filepath)  # Add to in-memory history\n",
    "            with open(self.history_file, 'a') as file:\n",
    "                file.write(filepath + \"\\n\")\n",
    "\n",
    "            # Clean the memory\n",
    "            del loader, data\n",
    "            gc.collect()\n",
    "\n",
    "            return 'Success'\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filepath}: {e}\")\n",
    "            with open(self.error_file, 'a') as file:\n",
    "                file.write(filepath + str(e) + \"\\n\")\n",
    "            return 'Error'\n",
    "        \n",
    "\n",
    "    def process_tabular(self, filepath: str):\n",
    "        \"\"\"\n",
    "        This Method processes a PDF file and puts the documents into the self.documents list\n",
    "        :param filepath: The path to the PDF file\n",
    "        :return: 'Success' if the file was processed, 'Skipped' if it was already processed\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Verify if the file was already processed\n",
    "            if filepath in self.processed_files:\n",
    "                print(f\"{filepath} already processed. Skipping.\")\n",
    "                return 'Skipped'\n",
    "\n",
    "            # Load the file with pandas\n",
    "            df = pd.read_excel(filepath)\n",
    "\n",
    "            # Build Documents with the information\n",
    "            documents_xls = []\n",
    "            for _, row in df.iterrows():\n",
    "                # Filtrar columnas que no estén nombradas (evitar \"Unnamed\")\n",
    "                valid_columns = [col for col in df.columns if not col.startswith('Unnamed')]\n",
    "                \n",
    "                # Comprobar que la fila tiene al menos un valor no nulo en las columnas válidas\n",
    "                if not row[valid_columns].isnull().all():\n",
    "                    # Crear un string para cada fila concatenando \"Columna: Valor\" para cada columna\n",
    "                    page_content = \"\\n\".join([f\"{col}: {row[col]}\" for col in valid_columns if pd.notnull(row[col])])\n",
    "\n",
    "                    # Agregar el string de la fila a la lista resultado\n",
    "                    documents_xls.append(Document(page_content=page_content, metadata={'source': filepath}))\n",
    "\n",
    "            # Filter the documents\n",
    "            documents_xls = self.filter_documents(documents_xls)\n",
    "\n",
    "            # Add metadata to the documents\n",
    "            documents_xls = self.add_metadata(documents_xls, type='tabular')\n",
    "\n",
    "            if not documents_xls:\n",
    "                return False\n",
    "            \n",
    "            # Save the processed data\n",
    "            self.documents.extend(documents_xls)\n",
    "\n",
    "            # If there are many documents, save and free the memory\n",
    "            if len(self.documents) > 50:\n",
    "                status_save = self.save_procceced_data_into_vector_store()\n",
    "                if status_save == 'Success':\n",
    "                    self.documents = []  # Clear documents to free memory\n",
    "                    print(\"Data was processed and saved\")\n",
    "\n",
    "            # Save the file in the history after processing\n",
    "            self.processed_files.add(filepath)  # Add to in-memory history\n",
    "            with open(self.history_file, 'a') as file:\n",
    "                file.write(filepath + \"\\n\")\n",
    "\n",
    "            # Clean the memory\n",
    "            del df\n",
    "            gc.collect()\n",
    "            return 'Success'\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filepath}: {e}\")\n",
    "            with open(self.error_file, 'a') as file:\n",
    "                file.write(filepath + str(e) + \"\\n\")\n",
    "            return 'Error'\n",
    "        \n",
    "    def save_procceced_data_into_vector_store(self):\n",
    "        \"\"\"\n",
    "        Saves the processed data into a vector store\n",
    "        :param documents_to_save: List of processed documents\n",
    "        :return: 'Success' if saved successfully\n",
    "        \"\"\"\n",
    "        # Save the data in the vector store\n",
    "        uuids = [str(uuid4()) for _ in range(len(self.documents))]\n",
    "\n",
    "        self.vector_store.add_documents(documents=self.documents, ids=uuids)\n",
    "\n",
    "        return 'Success'\n",
    "\n",
    "\n",
    "# Define the base path to process the data\n",
    "base_path = '/home/andromedalactea/freelance/AI_APEC_Develop/data/pdf'\n",
    "\n",
    "# Create an Object to process the data\n",
    "process_data = ProcessData()\n",
    "base_path = os.path.normpath(os.path.join(os.path.dirname(os.path.realpath(__file__)), base_path))\n",
    "\n",
    "# List of file extensions that can be converted to PDF\n",
    "convertible_to_pdf = ['.docx', '.doc', '.txt', '.html', '.htm', '.xml', '.rtf', '.pptx', '.ppt']\n",
    "tabular_files = ['.xlsx', '.xls']\n",
    "\n",
    "# Function to convert files to PDF using Pandoc\n",
    "def convert_to_pdf_pandoc(file_path, output_pdf_path):\n",
    "    try:\n",
    "        print(f\"Trying to convert with Pandoc: {file_path}\")\n",
    "        # Try converting using pypandoc\n",
    "        pypandoc.convert_file(file_path, 'pdf', outputfile=output_pdf_path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        print(f\"Error converting {file_path} to PDF with Pandoc.\")\n",
    "        print(traceback.format_exc())  # Log the detailed error trace\n",
    "        return False\n",
    "\n",
    "# Function to convert files to PDF using LibreOffice if Pandoc fails\n",
    "def convert_to_pdf_libreoffice(input_file, output_dir):\n",
    "    try:\n",
    "        print(f\"Trying to convert with LibreOffice: {input_file}\")\n",
    "        # Run LibreOffice in terminal to convert the file to PDF\n",
    "        subprocess.run(['libreoffice', '--headless', '--convert-to', 'pdf', '--outdir', output_dir, input_file], check=True)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error converting {input_file} to PDF with LibreOffice: {e}\")\n",
    "        return False\n",
    "\n",
    "# Function to log files that fail to convert to PDF\n",
    "def log_failed_file(file_path):\n",
    "    with open(process_data.error_files, 'a') as error_log:\n",
    "        error_log.write(file_path + '\\n')  # Add the failed file to the error log\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "    for filename in filenames:\n",
    "        # Extract the file extension\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "        # If the file is already a PDF, process it as usual\n",
    "        if file_extension == '.pdf':\n",
    "            pdf_path = os.path.join(dirpath, filename)\n",
    "            result = process_data.process_pdf(pdf_path)\n",
    "\n",
    "            if result == 'Success':\n",
    "                with open(process_data.history_file, 'r') as file:\n",
    "                    number_files = len(file.read().splitlines())\n",
    "                    print(\"Number of files processed: \", number_files)\n",
    "\n",
    "        # Process tabular files            \n",
    "        elif file_extension in tabular_files:\n",
    "            xls_path = os.path.join(dirpath, filename)\n",
    "            result = process_data.process_tabular(xls_path)\n",
    "\n",
    "            if result == 'Success':\n",
    "                with open(process_data.history_file, 'r') as file:\n",
    "                    number_files = len(file.read().splitlines())\n",
    "                    print(\"Number of files processed: \", number_files)\n",
    "\n",
    "        # If the file can be converted to PDF\n",
    "        elif file_extension in convertible_to_pdf:\n",
    "            original_file_path = os.path.join(dirpath, filename)\n",
    "            output_pdf_name = os.path.splitext(filename)[0] + '.pdf'\n",
    "            output_pdf_path = os.path.join(dirpath, output_pdf_name)\n",
    "\n",
    "            # Convert only if the PDF file doesn't already exist\n",
    "            if not os.path.exists(output_pdf_path):\n",
    "                # 1. Try with Pandoc\n",
    "                conversion_success = convert_to_pdf_pandoc(original_file_path, output_pdf_path)\n",
    "\n",
    "                # 2. If Pandoc fails, try with LibreOffice\n",
    "                if not conversion_success:\n",
    "                    print(f\"Trying alternative conversion for {filename} with LibreOffice\")\n",
    "                    conversion_success = convert_to_pdf_libreoffice(original_file_path, dirpath)\n",
    "\n",
    "                # If any conversion was successful, process the PDF\n",
    "                if conversion_success:\n",
    "                    result = process_data.process_pdf(output_pdf_path)\n",
    "\n",
    "                    if result == 'Success':\n",
    "                        with open(process_data.history_file, 'r') as file:\n",
    "                            number_files = len(file.read().splitlines())\n",
    "                            print(\"Number of files processed: \", number_files)\n",
    "                    else:\n",
    "                        print(f\"File {output_pdf_path} processed but not marked as successful.\")\n",
    "                else:\n",
    "                    # If all conversion attempts failed, log the file\n",
    "                    print(f\"All conversion methods failed for {original_file_path}. Logging error.\")\n",
    "                    log_failed_file(original_file_path)\n",
    "            else:\n",
    "                print(f\"{output_pdf_name} already exists. Skipping conversion.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
