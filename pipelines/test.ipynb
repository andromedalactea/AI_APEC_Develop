{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install  -r ../requirements_pipelines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the base path to process the data\n",
    "# base_path = '/mnt/i'\n",
    "\n",
    "# # Create an Object to process the data\n",
    "# process_data = ProcessData()\n",
    "\n",
    "# # Iterate over all files in a directory\n",
    "# for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "\n",
    "#     # Explore the files\n",
    "#     for filename in filenames:\n",
    "        \n",
    "#         # Extract the extension\n",
    "#         file_extension = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "#         if file_extension == '.pdf':\n",
    "            \n",
    "#             # Process the PDF file\n",
    "#             result = process_data.process_pdf(os.path.join(dirpath, filename))\n",
    "            \n",
    "#             # If the file was processed successfully\n",
    "#             if result == 'Success':\n",
    "#                 # How many files were processed\n",
    "#                 with open(process_data.history_file, 'r') as file:\n",
    "#                     number_files = len(file.read().splitlines())\n",
    "#                     print(\"Number of files processed: \", number_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python Imports\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Third party imports\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData:\n",
    "    def __init__(self,\n",
    "                 data_directory: str =  \"/home/apec_ai/AI_Develops/AI_APEC_Develop/data/APEC_ChromaDB_VectorStore\",\n",
    "                 history_file: str = \"/home/apec_ai/AI_Develops/AI_APEC_Develop/data/processed_files.txt\",\n",
    "                 error_file: str = \"/home/apec_ai/AI_Develops/AI_APEC_Develop/data/error_files.txt\",\n",
    "                 embedding_model: str = \"text-embedding-3-large\"):\n",
    "        \n",
    "        # List to store the documents temporarily\n",
    "        self.documents = []\n",
    "\n",
    "        # History file\n",
    "        os.makedirs(os.path.dirname(history_file), exist_ok=True)\n",
    "        self.history_file = history_file\n",
    "\n",
    "        # Error file\n",
    "        os.makedirs(os.path.dirname(error_file), exist_ok=True)\n",
    "        self.error_file = error_file\n",
    "\n",
    "        # Define the Database to store the embeddings\n",
    "        self.data_directory = data_directory\n",
    "\n",
    "        # Load already processed files into memory\n",
    "        self.processed_files = self.load_processed_files()\n",
    "\n",
    "        # Define the initial text splitter\n",
    "        self.text_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\",  # Use line breaks as separator\n",
    "            chunk_size=5000,\n",
    "            chunk_overlap=500,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "\n",
    "        # Embedding Model\n",
    "        self.embedding_openai = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Verify the data directory\n",
    "        os.makedirs(self.data_directory, exist_ok=True)  # Create the directory and any necessary parent directories\n",
    "        os.chmod(self.data_directory, 0o777) \n",
    "        # Vector Store Chroma\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=\"APEC_collection\",\n",
    "            embedding_function=self.embedding_openai,\n",
    "            persist_directory=self.data_directory,  \n",
    "        )\n",
    "\n",
    "    def load_processed_files(self):\n",
    "        \"\"\"\n",
    "        Loads the processed files from the history file into a list\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.history_file, 'r') as file:\n",
    "                return set(file.read().splitlines())\n",
    "        except FileNotFoundError:\n",
    "            return set()\n",
    "\n",
    "    def process_pdf(self, filepath: str):\n",
    "        \"\"\"\n",
    "        This Method processes a PDF file and puts the documents into the self.documents list\n",
    "        :param filepath: The path to the PDF file\n",
    "        :return: 'Success' if the file was processed, 'Skipped' if it was already processed\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Verify if the file was already processed\n",
    "            if filepath in self.processed_files:\n",
    "                print(f\"{filepath} already processed. Skipping.\")\n",
    "                return 'Skipped'\n",
    "\n",
    "            # Load the file\n",
    "            loader = UnstructuredPDFLoader(filepath)\n",
    "            data = loader.load()\n",
    "\n",
    "            # Split the text into chunks\n",
    "            documents_pdf = self.text_splitter.split_documents(data)\n",
    "            print(\"Number of documents:\", len(documents_pdf))\n",
    "\n",
    "            # Save the processed data\n",
    "            self.documents.extend(documents_pdf)\n",
    "\n",
    "            # If there are many documents, save and free the memory\n",
    "            if len(self.documents) > 50:\n",
    "                status_save = self.save_procceced_data_into_vector_store()\n",
    "                if status_save == 'Success':\n",
    "                    self.documents = []  # Clear documents to free memory\n",
    "                    print(\"Data was processed and saved\")\n",
    "\n",
    "            # Save the file in the history after processing\n",
    "            self.processed_files.add(filepath)  # Add to in-memory history\n",
    "            with open(self.history_file, 'a') as file:\n",
    "                file.write(filepath + \"\\n\")\n",
    "\n",
    "            return 'Success'\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filepath}: {e}\")\n",
    "            with open(self.error_file, 'a') as file:\n",
    "                file.write(filepath + str(e) + \"\\n\")\n",
    "            return 'Error'\n",
    "\n",
    "    def save_procceced_data_into_vector_store(self):\n",
    "        \"\"\"\n",
    "        Saves the processed data into a vector store\n",
    "        :param documents_to_save: List of processed documents\n",
    "        :return: 'Success' if saved successfully\n",
    "        \"\"\"\n",
    "        # Save the data in the vector store\n",
    "        self.vector_store.add_documents(documents=self.documents)\n",
    "\n",
    "        return 'Success'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the data to a Vector Store Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21145/480165359.py:35: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  self.embedding_openai = OpenAIEmbeddings(model=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Integra/Complete Integra 100, 500 configuration guide.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Integra/My Tank Info/m2021-integra-configuration-guide pg 48.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/ProGauge/M2051 LX 4 and LX Plus Configuration Manual.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Tech_Docs.PDF already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/AdobeReader6/Help/ENU/Reader.pdf already processed. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apec_ai/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file /mnt/i/ATG/Veeder-root tech docs/AdobeReader6/Reader/Messages/ENU/RdrMsgENU.pdf: Unable to get page count.\n",
      "Command Line Error: Incorrect password\n",
      "\n",
      "Number of documents: 0\n",
      "Number of files processed:  30\n",
      "Number of documents: 1\n",
      "Number of files processed:  31\n",
      "Number of documents: 0\n",
      "Number of files processed:  32\n",
      "Number of documents: 1\n",
      "Number of files processed:  33\n",
      "/mnt/i/ATG/Veeder-root tech docs/AdobeReader6/Resource/ENUtxt.pdf already processed. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/329839-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 13\n",
      "Number of files processed:  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/330583-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 8\n",
      "Number of files processed:  35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331651-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10\n",
      "Number of files processed:  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331793-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 7\n",
      "Number of files processed:  37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331794-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 7\n",
      "Number of files processed:  38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331940-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "Created a chunk of size 1133, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 15\n",
      "Data was processed and saved\n",
      "Number of files processed:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331940-002.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "Created a chunk of size 2837, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 11\n",
      "Number of files processed:  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331940-003.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "Created a chunk of size 1552, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 13\n",
      "Number of files processed:  41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1902, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4\n",
      "Number of files processed:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331940-005.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 8\n",
      "Number of files processed:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331940-006.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 15\n",
      "Data was processed and saved\n",
      "Number of files processed:  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/331940-008.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 13\n",
      "Number of files processed:  45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/332094-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2\n",
      "Number of files processed:  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/332280-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "Number of files processed:  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/332377-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2\n",
      "Number of files processed:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Drawings/332771-001.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "Number of files processed:  49\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-285.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-301.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-306.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-308.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-498.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-499.pdf already processed. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-567.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "Created a chunk of size 506, which is longer than the specified 500\n",
      "Created a chunk of size 585, which is longer than the specified 500\n",
      "Created a chunk of size 605, which is longer than the specified 500\n",
      "Created a chunk of size 509, which is longer than the specified 500\n",
      "Created a chunk of size 731, which is longer than the specified 500\n",
      "Created a chunk of size 583, which is longer than the specified 500\n",
      "Created a chunk of size 784, which is longer than the specified 500\n",
      "Created a chunk of size 561, which is longer than the specified 500\n",
      "Created a chunk of size 689, which is longer than the specified 500\n",
      "Created a chunk of size 610, which is longer than the specified 500\n",
      "Created a chunk of size 1859, which is longer than the specified 500\n",
      "Created a chunk of size 796, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 273\n",
      "Data was processed and saved\n",
      "Number of files processed:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 550, which is longer than the specified 500\n",
      "Created a chunk of size 720, which is longer than the specified 500\n",
      "Created a chunk of size 536, which is longer than the specified 500\n",
      "Created a chunk of size 2737, which is longer than the specified 500\n",
      "Created a chunk of size 536, which is longer than the specified 500\n",
      "Created a chunk of size 610, which is longer than the specified 500\n",
      "Created a chunk of size 705, which is longer than the specified 500\n",
      "Created a chunk of size 699, which is longer than the specified 500\n",
      "Created a chunk of size 573, which is longer than the specified 500\n",
      "Created a chunk of size 1096, which is longer than the specified 500\n",
      "Created a chunk of size 664, which is longer than the specified 500\n",
      "Created a chunk of size 689, which is longer than the specified 500\n",
      "Created a chunk of size 536, which is longer than the specified 500\n",
      "Created a chunk of size 524, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 279\n",
      "Data was processed and saved\n",
      "Number of files processed:  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 707, which is longer than the specified 500\n",
      "Created a chunk of size 585, which is longer than the specified 500\n",
      "Created a chunk of size 588, which is longer than the specified 500\n",
      "Created a chunk of size 608, which is longer than the specified 500\n",
      "Created a chunk of size 811, which is longer than the specified 500\n",
      "Created a chunk of size 612, which is longer than the specified 500\n",
      "Created a chunk of size 578, which is longer than the specified 500\n",
      "Created a chunk of size 2965, which is longer than the specified 500\n",
      "Created a chunk of size 2034, which is longer than the specified 500\n",
      "Created a chunk of size 964, which is longer than the specified 500\n",
      "Created a chunk of size 691, which is longer than the specified 500\n",
      "Created a chunk of size 614, which is longer than the specified 500\n",
      "Created a chunk of size 577, which is longer than the specified 500\n",
      "Created a chunk of size 593, which is longer than the specified 500\n",
      "Created a chunk of size 1260, which is longer than the specified 500\n",
      "Created a chunk of size 542, which is longer than the specified 500\n",
      "Created a chunk of size 727, which is longer than the specified 500\n",
      "Created a chunk of size 606, which is longer than the specified 500\n",
      "Created a chunk of size 557, which is longer than the specified 500\n",
      "Created a chunk of size 538, which is longer than the specified 500\n",
      "Created a chunk of size 596, which is longer than the specified 500\n",
      "Created a chunk of size 525, which is longer than the specified 500\n",
      "Created a chunk of size 507, which is longer than the specified 500\n",
      "Created a chunk of size 503, which is longer than the specified 500\n",
      "Created a chunk of size 503, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 393\n",
      "Data was processed and saved\n",
      "Number of files processed:  52\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-589.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-607.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-610.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-614.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-616.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-617.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-623.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-632.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-635.pdf already processed. Skipping.\n",
      "/mnt/i/ATG/Veeder-root tech docs/Manuals/576013-637.pdf already processed. Skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m file_extension \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_extension \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Process the PDF file\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# If the file was processed successfully\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuccess\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# How many files were processed\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m, in \u001b[0;36mProcessData.process_pdf\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Load the file\u001b[39;00m\n\u001b[1;32m     71\u001b[0m loader \u001b[38;5;241m=\u001b[39m UnstructuredPDFLoader(filepath)\n\u001b[0;32m---> 72\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Split the text into chunks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m documents_pdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter\u001b[38;5;241m.\u001b[39msplit_documents(data)\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:30\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/unstructured.py:107\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:74\u001b[0m, in \u001b[0;36mUnstructuredPDFLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/unstructured/documents/elements.py:605\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[0;32m--> 605\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    608\u001b[0m     regex_metadata: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m call_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/unstructured/file_utils/filetype.py:731\u001b[0m, in \u001b[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[0;32m--> 731\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m     params \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    733\u001b[0m     include_metadata \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/unstructured/file_utils/filetype.py:687\u001b[0m, in \u001b[0;36madd_metadata.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[0;32m--> 687\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    689\u001b[0m     include_metadata \u001b[38;5;241m=\u001b[39m call_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/unstructured/chunking/dispatch.py:74\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[1;32m     77\u001b[0m call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/unstructured/partition/pdf.py:206\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[0;34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, include_metadata, metadata_filename, metadata_last_modified, chunking_strategy, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, date_from_file_object, starting_page_number, extract_forms, form_extraction_skip_tables, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m exactly_one(filename\u001b[38;5;241m=\u001b[39mfilename, file\u001b[38;5;241m=\u001b[39mfile)\n\u001b[1;32m    204\u001b[0m languages \u001b[38;5;241m=\u001b[39m check_language_args(languages \u001b[38;5;129;01mor\u001b[39;00m [], ocr_languages)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf_or_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_from_file_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_from_file_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/unstructured/partition/pdf.py:342\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[0;34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, languages, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, date_from_file_object, starting_page_number, extract_forms, form_extraction_skip_tables, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m PartitionStrategy\u001b[38;5;241m.\u001b[39mOCR_ONLY:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;66;03m# NOTE(robinson): Catches file conversion warnings when running with PDFs\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m--> 342\u001b[0m         elements \u001b[38;5;241m=\u001b[39m \u001b[43m_partition_pdf_or_image_with_ocr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m            \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlast_modification_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m         out_elements \u001b[38;5;241m=\u001b[39m _process_uncategorized_text_elements(elements)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_elements\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/unstructured/partition/pdf.py:884\u001b[0m, in \u001b[0;36m_partition_pdf_or_image_with_ocr\u001b[0;34m(filename, file, include_page_breaks, languages, ocr_languages, is_image, metadata_last_modified, starting_page_number, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         elements\u001b[38;5;241m.\u001b[39mextend(page_elements)\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 884\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page_number, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    885\u001b[0m         convert_pdf_to_images(filename, file), start\u001b[38;5;241m=\u001b[39mstarting_page_number\n\u001b[1;32m    886\u001b[0m     ):\n\u001b[1;32m    887\u001b[0m         page_elements \u001b[38;5;241m=\u001b[39m _partition_pdf_or_image_with_ocr_from_image(\n\u001b[1;32m    888\u001b[0m             image\u001b[38;5;241m=\u001b[39mimage,\n\u001b[1;32m    889\u001b[0m             languages\u001b[38;5;241m=\u001b[39mlanguages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    895\u001b[0m         )\n\u001b[1;32m    896\u001b[0m         elements\u001b[38;5;241m.\u001b[39mextend(page_elements)\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/unstructured/partition/pdf_image/pdf_image_utils.py:416\u001b[0m, in \u001b[0;36mconvert_pdf_to_images\u001b[0;34m(filename, file, chunk_size)\u001b[0m\n\u001b[1;32m    410\u001b[0m     chunk_images \u001b[38;5;241m=\u001b[39m pdf2image\u001b[38;5;241m.\u001b[39mconvert_from_bytes(\n\u001b[1;32m    411\u001b[0m         f_bytes,\n\u001b[1;32m    412\u001b[0m         first_page\u001b[38;5;241m=\u001b[39mstart_page,\n\u001b[1;32m    413\u001b[0m         last_page\u001b[38;5;241m=\u001b[39mend_page,\n\u001b[1;32m    414\u001b[0m     )\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     chunk_images \u001b[38;5;241m=\u001b[39m \u001b[43mpdf2image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m chunk_images:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m image\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/pdf2image/pdf2image.py:127\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(poppler_path, PurePath):\n\u001b[1;32m    125\u001b[0m     poppler_path \u001b[38;5;241m=\u001b[39m poppler_path\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[0;32m--> 127\u001b[0m page_count \u001b[38;5;241m=\u001b[39m \u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mownerpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoppler_path\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[39;00m\n\u001b[1;32m    132\u001b[0m parsed_fmt, final_extension, parse_buffer_func, use_pdfcairo_format \u001b[38;5;241m=\u001b[39m _parse_format(\n\u001b[1;32m    133\u001b[0m     fmt, grayscale\n\u001b[1;32m    134\u001b[0m )\n",
      "File \u001b[0;32m~/AI_Develops/AI_APEC_Develop/.venv/lib/python3.10/site-packages/pdf2image/pdf2image.py:584\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[1;32m    581\u001b[0m proc \u001b[38;5;241m=\u001b[39m Popen(command, env\u001b[38;5;241m=\u001b[39menv, stdout\u001b[38;5;241m=\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39mPIPE)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     out, err \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired:\n\u001b[1;32m    586\u001b[0m     proc\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:2021\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2015\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2016\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2021\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the base path to process the data\n",
    "base_path = '/mnt/i'\n",
    "\n",
    "# Create an Object to process the data\n",
    "process_data = ProcessData()\n",
    "\n",
    "# Iterate over all files in a directory\n",
    "for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "\n",
    "    # Explore the files\n",
    "    for filename in filenames:\n",
    "        \n",
    "        # Extract the extension\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "        if file_extension == '.pdf':\n",
    "            \n",
    "            # Process the PDF file\n",
    "            result = process_data.process_pdf(os.path.join(dirpath, filename))\n",
    "            \n",
    "            # If the file was processed successfully\n",
    "            if result == 'Success':\n",
    "                # How many files were processed\n",
    "                with open(process_data.history_file, 'r') as file:\n",
    "                    number_files = len(file.read().splitlines())\n",
    "                    print(\"Number of files processed: \", number_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.753500] \n",
      "NOTE!\n",
      "You cannot reuse the same car wash panel media for multiple sites if they do not have the same softkey number. Only one tag can be entered; you must upload a copy of the image file and uniquely tag it for each site with different softkey numbers.\n",
      "When the image files have been uploaded and tagged, create a Carwash media set per the user guide. \n",
      "[{'source': '/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf'}]\n",
      "* [SIM=0.813229] \n",
      "NOTE!\n",
      "The car wash option panels can only be used for sites with three (3) or fewer car wash programs. On the point-of-sale (POS) system, the car wash programs must be assigned to softkey 1 (left), softkey 2 (center) and softkey 3 (right) to be associated with the corresponding tagged car wash panel in DX Promote. See Uploading and Tagging Images on page 3 to associate wash programs with option panels via tagging.\n",
      "Media Specifications \n",
      "[{'source': '/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf'}]\n",
      "* [SIM=0.940055] \n",
      "The car wash option panel displayed will be a custom image file downloaded with the car wash promotion, or a set of default images similar to the figure at right.\n",
      "In the example at right, the text Ultimate $13.99 is the wash options soft key label from the car wash prompts sent by the POS system. When custom panels are used the software will attempt to display only the price, but will use the entire label if parsing is not successful.\n",
      "NOTE! \n",
      "[{'source': '/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf'}]\n",
      "* [SIM=0.942606] \n",
      "Media Specifications\n",
      "The image file specifications for the enhanced car wash feature are a subset of those already used for the AX12 Enhanced and Anthem UX platforms. However, specific resolutions are required to fit the available screen areas. The image file specifications for this feature are provided below. \n",
      "[{'source': '/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf'}]\n",
      "[(Document(metadata={'source': '/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf'}, page_content='NOTE!\\nYou cannot reuse the same car wash panel media for multiple sites if they do not have the same softkey number. Only one tag can be entered; you must upload a copy of the image file and uniquely tag it for each site with different softkey numbers.\\nWhen the image files have been uploaded and tagged, create a Carwash media set per the user guide.'), 0.7535001635551453), (Document(metadata={'source': '/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf'}, page_content='NOTE!\\nThe car wash option panels can only be used for sites with three (3) or fewer car wash programs. On the point-of-sale (POS) system, the car wash programs must be assigned to softkey 1 (left), softkey 2 (center) and softkey 3 (right) to be associated with the corresponding tagged car wash panel in DX Promote. See Uploading and Tagging Images on page 3 to associate wash programs with option panels via tagging.\\nMedia Specifications'), 0.8132294416427612), (Document(metadata={'source': '/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf'}, page_content='The car wash option panel displayed will be a custom image file downloaded with the car wash promotion, or a set of default images similar to the figure at right.\\nIn the example at right, the text Ultimate $13.99 is the wash options soft key label from the car wash prompts sent by the POS system. When custom panels are used the software will attempt to display only the price, but will use the entire label if parsing is not successful.\\nNOTE!'), 0.9400553703308105), (Document(metadata={'source': '/mnt/i/DX_Promote_Enhanced_Car_Wash_for_AX12_and_Anthem_UX_Rev_02_1_.pdf'}, page_content='Media Specifications\\nThe image file specifications for the enhanced car wash feature are a subset of those already used for the AX12 Enhanced and Anthem UX platforms. However, specific resolutions are required to fit the available screen areas. The image file specifications for this feature are provided below.'), 0.9426055550575256)]\n"
     ]
    }
   ],
   "source": [
    "results = process_data.vector_store.similarity_search_with_score(\n",
    "    \"How to create a car wash media set\", k=4\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] \\n{res.page_content} \\n[{res.metadata}]\")\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
